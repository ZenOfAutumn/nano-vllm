# ModelRunner 执行流程图详解

## 整体架构流程

```
                          ┌─────────────────┐
                          │  LLM Engine     │
                          │  (上层调用)      │
                          └────────┬────────┘
                                   │
                                   ▼
                ┌──────────────────────────────────┐
                │   ModelRunner.__init__()         │
                │   初始化模型运行器                │
                └──────────────────────────────────┘
                                   │
                ┌──────────────────┼──────────────────────┐
                │                  │                      │
        ┌───────▼───────┐ ┌────────▼────────┐ ┌────────▼────────┐
        │ 初始化分布式   │ │  加载模型和权重  │ │   KV缓存分配    │
        │ (NCCL)        │ │  (Qwen3模型)    │ │   (GPU显存)     │
        └───────┬───────┘ └────────┬────────┘ └────────┬────────┘
                │                  │                  │
        ┌───────▼────────────────────────────────────▼────────┐
        │  是否使用 CUDA Graph?                               │
        │  (enforce_eager 参数检查)                           │
        └───────┬────────────────────────────────────┬────────┘
                │ No (推荐)                       Yes │
                │                                     │
        ┌───────▼──────────────────┐    ┌────────────▼────────┐
        │ capture_cudagraph()       │    │  跳过CUDA Graph     │
        │ 预录制CUDA图 (1,2,4,8...)│    │  使用Eager Mode    │
        └───────┬──────────────────┘    └────────────┬────────┘
                │                                     │
        ┌───────▼──────────────────────────────────────▼──────┐
        │  多进程模式检查                                      │
        │  (tensor_parallel_size > 1)                         │
        └───────┬──────────────────────────────────┬──────┘
                │ Yes                        No  │
                │                               │
        ┌───────▼─────────────┐ ┌──────────────────────────▼────┐
        │  主进程rank=0:      │ │  单进程模式                   │
        │  创建共享内存       │ │  只执行主进程逻辑             │
        │  sync barrier       │ │                              │
        │                     │ │  返回执行run()               │
        │  从进程rank>0:      │ │                              │
        │  连接共享内存       │ └──────────────────────┬───────┘
        │  进入loop()循环     │                       │
        │  (等待方法调用)     │                       │
        └──────────────────────┘                       │
                                                      │
                            ┌─────────────────────────┘
                            │
                            ▼
                   ┌──────────────────┐
                   │  run()            │
                   │ (主推理循环)      │
                   └──────────────────┘
```

---

## run() 推理主循环流程

```
                     ┌─────────────────────┐
                     │  run(seqs, phase)   │
                     │ 输入: 序列列表      │
                     │       是否prefill   │
                     └────────────┬────────┘
                                  │
                    ┌─────────────┴─────────────────┐
                    │ 根据推理阶段选择不同的准备方式 │
                    └─────────────┬─────────────────┘
                                  │
                    ┌─────────────┴─────────────────┐
                    │                               │
           ┌────────▼─────────┐      ┌─────────────▼────────┐
           │ is_prefill=True  │      │  is_prefill=False    │
           │ (前填充阶段)      │      │  (解码阶段)          │
           └────────┬─────────┘      └─────────────┬────────┘
                    │                               │
           ┌────────▼──────────────────┐ ┌────────────▼────────────┐
           │ prepare_prefill(seqs)     │ │ prepare_decode(seqs)    │
           │                          │ │                        │
           │ 【功能】:                │ │ 【功能】:              │
           │ • 提取所有未缓存token   │ │ • 只取最后一个token    │
           │ • 计算位置信息          │ │ • 位置=序列长度-1     │
           │ • 构建Flash Attn参数    │ │ • 每个序列一个新token  │
           │ • 分配KV缓存槽位        │ │ • 构建block_tables     │
           │                          │ │                        │
           │ 输出: (input_ids,       │ │ 输出: (input_ids,      │
           │       positions)         │ │       positions)       │
           └────────┬──────────────────┘ └────────────┬────────────┘
                    │                                 │
                    └───────────────┬───────────────┘
                                    │
                       ┌────────────▼──────────────┐
                       │ prepare_sample(seqs)      │
                       │ 获取采样温度参数           │
                       │ (仅主进程 rank=0)        │
                       └────────────┬──────────────┘
                                    │
                       ┌────────────▼──────────────┐
                       │ run_model()               │
                       │ 执行模型前向传播           │
                       └────────────┬──────────────┘
                                    │
                       ┌────────────▼──────────────┐
                       │ sampler()                 │
                       │ 采样生成token             │
                       │ (仅主进程)               │
                       └────────────┬──────────────┘
                                    │
                       ┌────────────▼──────────────┐
                       │ reset_context()           │
                       │ 清空执行上下文             │
                       └────────────┬──────────────┘
                                    │
                                    ▼
                       ┌────────────────────────┐
                       │ return token_ids       │
                       │ 返回生成的token列表    │
                       └────────────────────────┘
```

---

## run_model() 模型执行流程

```
                  ┌──────────────────────────────┐
                  │ @torch.inference_mode()      │
                  │ run_model(input_ids, pos)    │
                  │ 禁用梯度计算，优化推理性能   │
                  └──────────────────────────────┘
                                 │
              ┌──────────────────┴──────────────────┐
              │  检查是否可以使用CUDA Graph        │
              │  (优化执行速度)                    │
              └──────────────────┬──────────────────┘
                                 │
    ┌────────────────────────────┴────────────────────────────┐
    │                                                          │
   YES  │  条件:                                          NO  │
   (使用│  • is_prefill=True                                   │
   Eager│  • enforce_eager=True                               │
   Mode)│  • batch_size > 512                                 │
    │                                                          │
    ┌───▼─────────────────────────────┐    ┌────────────────▼────┐
    │ 直接执行模型                      │    │  使用预录制CUDA Graph   │
    │ model.forward()                  │    │                        │
    │ (动态图执行)                      │    │  1. 获取合适的Graph   │
    │                                  │    │     (batch_size)      │
    │ 返回 logits                       │    │  2. 从context获取数据 │
    └───┬────────────────────────────┘    │  3. 更新graph变量    │
        │                                  │     - input_ids        │
        │                                  │     - positions        │
        │                                  │     - slot_mapping     │
        │                                  │     - context_lens     │
        │                                  │     - block_tables     │
        │                                  │  4. graph.replay()     │
        │                                  │  5. 返回outputs        │
        │                                  └────────────┬───────────┘
        │                                               │
        └──────────────────────┬──────────────────────┘
                               │
                    ┌──────────▼──────────┐
                    │ compute_logits()    │
                    │ 计算最终logits      │
                    └──────────┬──────────┘
                               │
                               ▼
                    ┌──────────────────────┐
                    │  return logits       │
                    │  返回预测分布        │
                    └──────────────────────┘
```

---

## Prefill vs Decode 对比

| 特性 | Prefill 阶段 | Decode 阶段 |
|------|-------------|-----------|
| **目的** | 处理用户输入的所有tokens | 逐个生成输出tokens |
| | 构建KV缓存 | 利用KV缓存加速 |
| **输入数据** | 所有未缓存的token | 只有最后一个token |
| | 可能很长 (最长max_model_len) | 长度固定为1 |
| | 来自多个序列 | 来自多个序列 |
| **执行方式** | 使用Eager Mode (动态图) | 优先使用CUDA Graph (预录) |
| | Flash Attention优化 | 如果batch>512用Eager mode |
| | 并行处理多个序列 | 高性能、低延迟 |
| **计算复杂度** | O(N²) for attention | O(N) for attention |
| | (N=sum of seq lengths) | (N=batch size) |
| **KV缓存操作** | 写入新KV值 | 读取所有历史KV |
| | 分配新block | 追加写入一个新KV |
| | 可能触发块替换 | 使用已分配的block |

---

## 初始化流程详解

```
                        __init__()初始化流程

                                 │
                ┌────────────────┼────────────────┐
                │                │                │
        ┌───────▼──────┐ ┌──────▼──────┐ ┌──────▼──────┐
        │初始化分布式   │ │加载模型      │ │准备GPU环境  │
        │(dist.init)   │ │(load_model)  │ │(set_device)│
        └───────┬──────┘ └──────┬──────┘ └──────┬──────┘
                │                │              │
                └────────────────┼──────────────┘
                                 │
                ┌────────────────┴────────────────┐
                │                                 │
        ┌───────▼──────────┐         ┌──────────▼─────┐
        │warmup_model()    │         │allocate_kv_    │
        │预热GPU           │         │cache()         │
        │执行dummy forward │         │分配KV缓存张量  │
        │测量内存使用      │         │绑定到各层      │
        │清理缓存          │         │                │
        └───────┬──────────┘         └──────────┬─────┘
                │                               │
                └───────────────┬───────────────┘
                                │
                    ┌───────────▼────────────┐
                    │ enforce_eager?         │
                    └───────────┬────────┬───┘
                                │ No     │ Yes
                    ┌───────────▼─┐    ┌─▼──────────┐
                    │capture_cuda │    │跳过CUDA    │
                    │graph()      │    │Graph录制   │
                    │预录制多个   │    └────────────┘
                    │大小的graph  │          │
                    └───────────┬─┘          │
                                │           │
                    ┌───────────┴───────────┘
                    │
        ┌───────────▼────────────────────┐
        │多进程模式检查                   │
        │tensor_parallel_size > 1        │
        └───────────┬────────────────┬───┘
                    │ Yes        No  │
            ┌───────▼──┐    ┌────────▼────┐
            │创建SharedMemory
            │主进程创建     │完成初始化  │
            │从进程连接     │            │
            │从进程进入loop()   │
            └───────┬──┘    └────────┬────┘
                    │               │
                    └───────┬───────┘
                            │
                    ┌───────▼──────┐
                    │初始化完成    │
                    │等待run()调用 │
                    └──────────────┘
```

---

## 分布式多进程通信流程

多GPU张量并行模式 (tensor_parallel_size > 1)

```
                         ┌──────────────┐
                         │  主进程(rank=0)
                         │  执行run()    │
                         └────────┬─────┘
                                  │
                                  ▼
                          ┌───────────────┐
                          │ call(method)  │
                          │ 检查rank      │
                          └───────┬───────┘
                                  │
                    ┌─────────────┴─────────────┐
                    │ rank=0?                   │
                    └─────────────┬─────────────┘
                         Yes      │      No
                                  │
                        ┌─────────▼────────┐
                        │write_shm()       │
                        │序列化方法名和参数│
                        │写到共享内存      │
                        │向所有从进程发信号│
                        └─────────┬────────┘
                                  │
                        ┌─────────▼──────────────┐
                        │ for each worker_event: │
                        │   event.set()          │
                        │ (唤醒所有从进程)      │
                        └─────────┬──────────────┘
                                  │
                    ┌─────────────┴──────────────┐
                    │                            │
            ┌───────▼─────────────┐  ┌──────────▼────┐
            │从进程 loop()        │  │主进程继续执行 │
            │event.wait()         │  │该方法         │
            │read_shm()           │  │               │
            │反序列化方法和参数   │  │同时从进程:    │
            │call()执行方法       │  │call()执行相同 │
            │event.clear()        │  │的方法         │
            └───────┬─────────────┘  └──────────┬────┘
                    │                           │
                    └───────────┬───────────────┘
                                │
                    ┌───────────▼──────────────┐
                    │执行完成，同步GPU操作   │
                    │torch.cuda.synchronize()│
                    │返回结果(仅主进程有效) │
                    └───────────┬──────────────┘
                                │
                                ▼
                    ┌──────────────────────────┐
                    │返回推理结果              │
                    │token_ids (from main)    │
                    │None (from worker)       │
                    └──────────────────────────┘
```

---

## KV缓存分配流程

allocate_kv_cache() 详细流程

```
                   ┌──────────────────────┐
                   │ allocate_kv_cache()  │
                   │ 分配GPU的KV缓存      │
                   └──────────────┬───────┘
                                  │
                    ┌─────────────▼──────────────┐
                    │ 1. 查询GPU内存信息          │
                    │    torch.cuda.mem_get_info()
                    │    -> (free, total)       │
                    │ 2. 查询内存统计            │
                    │    memory_stats()         │
                    │    -> peak, current       │
                    └─────────────┬──────────────┘
                                  │
                    ┌─────────────▼──────────────┐
                    │ 3. 计算单个块的大小        │
                    │                            │
                    │  block_bytes = 2 * (K+V)  │
                    │  * num_hidden_layers      │
                    │  * block_size             │
                    │  * num_kv_heads           │
                    │  * head_dim               │
                    │  * dtype.itemsize         │
                    │                            │
                    │ 例: 2 * 32 * 256 * 1 * 128│
                    │     * 2 = 4194304 bytes   │
                    └─────────────┬──────────────┘
                                  │
                    ┌─────────────▼──────────────┐
                    │ 4. 计算最大块数            │
                    │                            │
                    │ num_blocks =              │
                    │   (total * util           │
                    │    - used - peak + cur)  │
                    │   / block_bytes          │
                    │                            │
                    │ 例: (40GB*0.9-10-5+3)    │
                    │     / 4MB = 7000 blocks  │
                    └─────────────┬──────────────┘
                                  │
                    ┌─────────────▼──────────────┐
                    │ 5. 创建KV缓存张量          │
                    │                            │
                    │ shape:                    │
                    │ (2, layers, blocks,       │
                    │  block_size, heads, dim) │
                    │                            │
                    │ 例: (2, 32, 7000,        │
                    │     256, 1, 128)         │
                    └─────────────┬──────────────┘
                                  │
                    ┌─────────────▼──────────────┐
                    │ 6. 分配给各层注意力模块    │
                    │                            │
                    │ for each layer:           │
                    │   layer.k_cache =        │
                    │     kv_cache[0, layer]   │
                    │   layer.v_cache =        │
                    │     kv_cache[1, layer]   │
                    └─────────────┬──────────────┘
                                  │
                                  ▼
                    ┌─────────────────────────┐
                    │完成KV缓存分配           │
                    │后续可使用这些缓存       │
                    └─────────────────────────┘
```

---

## CUDA Graph 录制流程

capture_cudagraph() 预录制多个大小的计算图

```
                  ┌─────────────────────┐
                  │ capture_cudagraph() │
                  │ 为decode阶段优化    │
                  └──────────┬──────────┘
                             │
              ┌──────────────▼─────────────────┐
              │ 1. 创建CUDA Graph变量张量      │
              │    input_ids: (max_bs,)       │
              │    positions: (max_bs,)       │
              │    slot_mapping: (max_bs,)    │
              │    context_lens: (max_bs,)    │
              │    block_tables: (max_bs, mb) │
              │    outputs: (max_bs, hidden)  │
              └──────────────┬─────────────────┘
                             │
              ┌──────────────▼─────────────────┐
              │ 2. 确定要录制的batch sizes     │
              │    [1, 2, 4, 8, 16..., max]   │
              │                                │
              │    例: 如果max_bs=512         │
              │    -> [1,2,4,8,16,32,...,512]│
              └──────────────┬─────────────────┘
                             │
              ┌──────────────▼─────────────────┐
              │ 3. 倒序录制(大→小)            │
              │    这样能复用内存池            │
              └──────────────┬─────────────────┘
                             │
              for each bs in reversed(graph_bs):
                             │
                ┌────────────▼──────────────┐
                │ 创建CUDAGraph对象        │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │ 设置执行上下文            │
                │ set_context(False, ...)  │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │ 热启动(warmup):          │
                │ outputs[:bs] =           │
                │   model(input[:bs], ...)│
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │ 开始录制:                 │
                │ with cuda.graph(g):     │
                │   outputs[:bs] =        │
                │     model(...)          │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │ 如果第一个图:            │
                │   创建memory_pool()     │
                │   供后续图复用           │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │ 保存该图:                 │
                │ graphs[bs] = graph       │
                └────────────┬──────────────┘
                             │
                ┌────────────▼──────────────┐
                │ GPU同步:                  │
                │ cuda.synchronize()       │
                └────────────┬──────────────┘
                             │
              (循环到下一个bs)
                             │
              ┌──────────────▼─────────────────┐
              │ 4. 保存图变量供后续重播        │
              │    graph_vars = dict(...)      │
              │                                │
              │    后续在run_model()中:        │
              │    • 更新graph_vars中的数据   │
              │    • graph.replay()重播       │
              │    • 直接获取输出             │
              └──────────────┬─────────────────┘
                             │
                             ▼
              ┌─────────────────────────────┐
              │ CUDA Graph录制完成          │
              │ decode阶段可直接使用        │
              │ 性能提升:减少CPU开销        │
              └─────────────────────────────┘
```

---

## 数据准备流程对比

### prepare_prefill() vs prepare_decode()

#### Prefill准备 - prepare_prefill()

**输入:**
```
[
  Seq0: token_ids=[101,102,103,104,105]  num_cached=0
  Seq1: token_ids=[201,202]               num_cached=0
  Seq2: token_ids=[301,302,303]           num_cached=1  (已缓存一个)
]
```

**处理步骤:**

1. **提取未缓存tokens**
   ```
   input_ids = [101,102,103,104,105, 201,202, 302,303]
   ```

2. **计算位置信息**
   ```
   positions = [0,1,2,3,4, 0,1, 1,2]
   ```

3. **Flash Attention参数 (包含序列边界信息)**
   ```
   cu_seqlens_q = [0, 5, 7, 9]
   cu_seqlens_k = [0, 5, 7, 9]
   max_seqlen_q = 5, max_seqlen_k = 5
   ```

4. **KV缓存槽位映射 (新token→block位置)**
   ```
   slot_mapping = [b0*256+0, b0*256+1, ..., b2*256+128, b2*256+129]
   ```

**输出:** (input_ids张量, positions张量)

---

#### Decode准备 - prepare_decode()

**输入:**
```
[
  Seq0: 已生成5个token (最后一个token_id=105)
  Seq1: 已生成2个token (最后一个token_id=202)
  Seq2: 已生成3个token (最后一个token_id=303)
]
```

**处理步骤:**

1. **只取最后一个token**
   ```
   input_ids = [105, 202, 303]
   ```

2. **位置是序列长度-1**
   ```
   positions = [4, 1, 2]
   ```

3. **上下文长度 (用于相对位置)**
   ```
   context_lens = [5, 2, 3]
   ```

4. **新token的KV缓存槽位**
   ```
   slot_mapping = [
       block_table[Seq0][-1]*256 + 4,  # Seq0最后一块的第5个位置
       block_table[Seq1][-1]*256 + 1,  # Seq1最后一块的第2个位置
       block_table[Seq2][-1]*256 + 2,  # Seq2最后一块的第3个位置
     ]
   ```

5. **block_tables (所有已分配的blocks)**
   ```
   [[b0_0, b0_1, -1],
    [b1_0, -1, -1],
    [b2_0, b2_1, -1]]
   ```

**输出:** (input_ids张量, positions张量)

---

## 完整推理周期

### 第一步: Prefill(处理输入提示词)

**输入:** `["Hello world"]`
→ Tokenize: `[101, 7592, 3186, 102]`

```
┌─────────────────────────────────┐
│ run([Seq0: [101,7592,3186,102]], │
│     is_prefill=True)             │
└────────┬────────────────────────┘
         │
  ┌──────▼──────────┐
  │prepare_prefill()│
  │提取所有token    │
  │计算位置和KV槽位 │
  └──────┬──────────┘
         │
  ┌──────▼──────────────────┐
  │run_model(input, pos)     │
  │执行Eager Mode           │
  │计算所有层的注意力        │
  │KV缓存写入GPU显存         │
  │输出logits: (1, vocab)   │
  └──────┬──────────────────┘
         │
  ┌──────▼──────────────────┐
  │sampler(logits, temp)     │
  │采样生成第一个输出token   │
  │例如生成: 2022 (word: "how")
  └──────┬──────────────────┘
         │
         ▼ 返回 [2022]

更新序列: Seq0 = [101,7592,3186,102,2022]
```

---

### 第二步: Decode(生成第一个输出)

```
┌──────────────────────────────────────┐
│ run([Seq0: [101,...,102,2022]],       │
│     is_prefill=False)                │
│ (已缓存前4个token, 需生成第5个)       │
└────────┬─────────────────────────────┘
         │
  ┌──────▼──────────────┐
  │prepare_decode()     │
  │只取最后token(2022)  │
  │位置=4               │
  │上下文长度=5         │
  │读KV缓存前4个tokens  │
  └──────┬──────────────┘
         │
  ┌──────▼────────────────────────┐
  │run_model(input=[2022], pos=4)  │
  │使用CUDA Graph (如可用)         │
  │只计算第5个位置的注意力         │
  │读取KV缓存,追加写一个新token   │
  │输出logits: (1, vocab)         │
  └──────┬────────────────────────┘
         │
  ┌──────▼──────────────┐
  │sampler(logits)      │
  │采样生成下一个token  │
  │例如生成: 2045       │
  └──────┬──────────────┘
         │
         ▼ 返回 [2045]

更新序列: Seq0 = [101,...,102,2022,2045]
```

---

### 循环重复

持续运行decode阶段，直到:
- 生成EOS token
- 达到最大长度限制
- 用户停止

---

## 核心要点总结

| 模块 | 功能 | 关键点 |
|------|------|--------|
| **__init__()** | 初始化ModelRunner | 分布式初始化、权重加载、KV缓存分配、CUDA Graph预录 |
| **run()** | 推理主循环 | 根据阶段调用prepare_prefill或prepare_decode |
| **prepare_prefill()** | Prefill数据准备 | 提取所有未缓存token，构建Flash Attn参数 |
| **prepare_decode()** | Decode数据准备 | 只取最后一个token，使用KV缓存 |
| **run_model()** | 模型执行 | 使用CUDA Graph或Eager Mode执行 |
| **allocate_kv_cache()** | KV缓存分配 | 计算块数、创建张量、分配给各层 |
| **capture_cudagraph()** | CUDA Graph录制 | 预录制多个batch size的计算图 |

---

**文档更新时间:** 2026年1月27日
**适用版本:** nano-vllm
**文档类型:** 技术架构详解

