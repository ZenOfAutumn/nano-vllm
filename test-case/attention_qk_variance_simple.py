"""
Attention ä¸­ Q å’Œ K è®¡ç®—ç»“æœæ–¹å·®åˆ†æ

è¿™ä¸ªç¤ºä¾‹å±•ç¤ºäº†ï¼š
1. Q @ K^T ç‚¹ç§¯ç»“æœçš„æ–¹å·®å¦‚ä½•éšç»´åº¦ (head_dim) å¢é•¿è€Œå˜åŒ–
2. ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾å› å­ (scale = 1/sqrt(d))
3. ç¼©æ”¾å‰åæ–¹å·®çš„å¯¹æ¯”
"""

import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np


def analyze_qk_variance():
    """
    åˆ†æ Q-K ç‚¹ç§¯ç»“æœçš„æ–¹å·®å˜åŒ–
    """
    # ç»´åº¦åˆ—è¡¨ï¼šä» 8 åˆ° 256ï¼Œæµ‹è¯•ä¸åŒçš„ head_dim å€¼
    head_dims = np.array([8, 16, 32, 64, 128, 256])

    # ç”¨äºå­˜å‚¨æ— ç¼©æ”¾æ—¶çš„æ–¹å·®ç»“æœ
    variances_without_scale = []
    # ç”¨äºå­˜å‚¨ç¼©æ”¾åï¼ˆscale=1/âˆšdï¼‰çš„æ–¹å·®ç»“æœ
    variances_with_scale = []
    # ç”¨äºå­˜å‚¨ç†è®ºå€¼ï¼ˆæ–¹å·® = head_dimï¼‰
    theoretical_variances = []

    # æ¯ä¸ªç»´åº¦è¿›è¡Œå¤šæ¬¡å®éªŒï¼Œå–å¹³å‡å€¼ä»¥ä¿è¯ç»Ÿè®¡æ„ä¹‰
    num_experiments = 1000

    # æ‰“å°åˆ†æå¼€å§‹æç¤º
    print("\nå¼€å§‹åˆ†æ Q-K ç‚¹ç§¯æ–¹å·®éšç»´åº¦çš„å˜åŒ–...")
    print("=" * 80)

    # å¯¹æ¯ä¸ª head_dim è¿›è¡Œåˆ†æ
    for head_dim in head_dims:
        # å‡è®¾ Q å’Œ K éƒ½ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–
        # è¿™æ˜¯ Transformer ä¸­å¸¸è§çš„åšæ³•ï¼ˆå³ N(0, 1)ï¼‰

        # åºåˆ—é•¿åº¦ï¼ˆæ¨¡æ‹Ÿ Attention ä¸­çš„åºåˆ—ï¼‰
        seq_len = 32

        # ä¸´æ—¶å­˜å‚¨æœ¬æ¬¡ç»´åº¦ä¸‹çš„æ‰€æœ‰å®éªŒæ–¹å·®ï¼ˆæ— ç¼©æ”¾ï¼‰
        variances_no_scale = []
        # ä¸´æ—¶å­˜å‚¨æœ¬æ¬¡ç»´åº¦ä¸‹çš„æ‰€æœ‰å®éªŒæ–¹å·®ï¼ˆç¼©æ”¾åï¼‰
        variances_scaled = []

        # è¿›è¡Œå¤šæ¬¡å®éªŒä»¥è·å¾—ç¨³å®šçš„ç»Ÿè®¡ç»“æœ
        for _ in range(num_experiments):
            # Q: (seq_len, head_dim) - æŸ¥è¯¢çŸ©é˜µï¼Œæ¯ä¸ªå€¼ä» N(0,1) é‡‡æ ·
            # K: (seq_len, head_dim) - é”®çŸ©é˜µï¼Œæ¯ä¸ªå€¼ä» N(0,1) é‡‡æ ·
            Q = np.random.normal(0, 1, (seq_len, head_dim))
            K = np.random.normal(0, 1, (seq_len, head_dim))

            # è®¡ç®— Q @ K^T å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°ï¼ˆæœªç¼©æ”¾ï¼‰
            # ç»“æœå½¢çŠ¶ï¼š(seq_len, seq_len)
            # æ¯ä¸ªå…ƒç´ æ˜¯ head_dim ä¸ªç‹¬ç«‹éšæœºå˜é‡ä¹˜ç§¯çš„å’Œ
            scores_no_scale = Q @ K.T

            # è®¡ç®—æ— ç¼©æ”¾æƒ…å†µä¸‹çš„æ–¹å·®
            # è¿™ä¸ªå€¼ä¼šéš head_dim çº¿æ€§å¢é•¿
            var_no_scale = np.var(scores_no_scale)
            variances_no_scale.append(var_no_scale)

            # è®¡ç®—ç¼©æ”¾å› å­ scale = 1/âˆšd
            # è¿™æ˜¯ Transformer ä¸­æ ‡å‡†åšæ³•ï¼Œç”¨äºé˜²æ­¢æ–¹å·®çˆ†ç‚¸
            scale = 1.0 / np.sqrt(head_dim)
            # åº”ç”¨ç¼©æ”¾å› å­åˆ°åˆ†æ•°
            # è¿™æ ·å¯ä»¥å°†æ–¹å·®ä» d ç¼©æ”¾åˆ° 1
            scores_scaled = scores_no_scale * scale

            # è®¡ç®—ç¼©æ”¾åçš„æ–¹å·®
            # ç†è®ºä¸Šåº”è¯¥æ¥è¿‘ 1.0
            var_scaled = np.var(scores_scaled)
            variances_scaled.append(var_scaled)

        # å¯¹æ‰€æœ‰å®éªŒç»“æœå–å¹³å‡ï¼Œå¾—åˆ°è¯¥ç»´åº¦ä¸‹çš„å¹³å‡æ–¹å·®ï¼ˆæ— ç¼©æ”¾ï¼‰
        avg_var_no_scale = np.mean(variances_no_scale)
        # å¯¹æ‰€æœ‰å®éªŒç»“æœå–å¹³å‡ï¼Œå¾—åˆ°è¯¥ç»´åº¦ä¸‹çš„å¹³å‡æ–¹å·®ï¼ˆç¼©æ”¾åï¼‰
        avg_var_scaled = np.mean(variances_scaled)

        # å°†å¹³å‡æ–¹å·®æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ï¼ˆæ— ç¼©æ”¾ï¼‰
        variances_without_scale.append(avg_var_no_scale)
        # å°†å¹³å‡æ–¹å·®æ·»åŠ åˆ°ç»“æœåˆ—è¡¨ï¼ˆç¼©æ”¾åï¼‰
        variances_with_scale.append(avg_var_scaled)

        # ç†è®ºæ–¹å·®ï¼ˆæ— ç¼©æ”¾ï¼‰ï¼šQ @ K^T ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ head_dim ä¸ªç‹¬ç«‹éšæœºå˜é‡çš„å’Œ
        # å½“ Q, K ~ N(0, 1) æ—¶ï¼š
        #   E[Q_i * K_i] = 0
        #   E[(Q_i * K_i)^2] = 1
        #   Var[Î£(Q_i * K_i)] = head_dimï¼ˆç‹¬ç«‹éšæœºå˜é‡ä¹‹å’Œçš„æ–¹å·®ï¼‰
        theoretical_var_no_scale = head_dim
        # ç†è®ºæ–¹å·®ï¼ˆç¼©æ”¾åï¼‰ï¼šç”±äºç¼©æ”¾å› å­æ˜¯ 1/âˆšdï¼Œæ–¹å·®ä¼šè¢« (1/âˆšd)Â² = 1/d å€ç¼©æ”¾
        # æ‰€ä»¥ Var[scaled_scores] = Var[scores / âˆšd] = (1/d) * Var[scores] = (1/d) * d = 1
        theoretical_var_scaled = 1.0

        # å°†ç†è®ºæ— ç¼©æ”¾æ–¹å·®æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        theoretical_variances.append(theoretical_var_no_scale)

        # æ‰“å°è¯¥ç»´åº¦çš„åˆ†æç»“æœ
        print(f"\nhead_dim = {head_dim:3d}")
        print(f"  â”Œâ”€ æ— ç¼©æ”¾æ–¹å·®:  {avg_var_no_scale:10.2f}  (ç†è®ºå€¼: {theoretical_var_no_scale:10.2f})")
        print(f"  â””â”€ ç¼©æ”¾åæ–¹å·®:  {avg_var_scaled:10.2f}  (ç†è®ºå€¼: {theoretical_var_scaled:10.2f})")

    # è¿”å›å››ä¸ªåˆ—è¡¨ï¼šç»´åº¦ã€æ— ç¼©æ”¾æ–¹å·®ã€ç¼©æ”¾åæ–¹å·®ã€ç†è®ºæ–¹å·®
    return head_dims, variances_without_scale, variances_with_scale, theoretical_variances


def plot_with_ascii():
    """
    ä½¿ç”¨ ASCII å­—ç¬¦ç»˜åˆ¶ç®€å•çš„æ–‡æœ¬å›¾è¡¨
    """
    # æ‰“å°æ ‡é¢˜
    print("\n" + "=" * 80)
    print("ASCII è‰ºæœ¯å›¾è¡¨ï¼šæ— ç¼©æ”¾æ—¶æ–¹å·®éšç»´åº¦å¢é•¿")
    print("=" * 80)

    # å®šä¹‰è¦ç»˜åˆ¶çš„ head_dim å€¼
    head_dims = np.array([8, 16, 32, 64, 128, 256])
    # æ³¨é‡Šï¼šç†è®ºå€¼ä¸º æ–¹å·® = head_dimï¼ˆçº¿æ€§å…³ç³»ï¼‰

    # æ‰“å°åæ ‡è½´æ ‡ç­¾
    print("\næ–¹å·®å€¼ â†‘")
    print("    |")

    # ç”¨äºç¡®å®šå›¾è¡¨é«˜åº¦çš„æœ€å¤§å€¼
    max_var = 256

    # ä»ä¸Šåˆ°ä¸‹éå†æ¯ä¸ªæ–¹å·®å€¼çº§åˆ«ï¼ˆ256, 192, ..., 0ï¼‰
    for var_value in [256, 192, 128, 64, 32, 16, 8, 0]:
        # ä¸ºé¡¶éƒ¨ï¼ˆ256ï¼‰ç‰¹æ®Šå¤„ç†
        if var_value == 256:
            label = "256 |"
        # ä¸ºåº•éƒ¨ï¼ˆ0ï¼‰ç‰¹æ®Šå¤„ç†
        elif var_value == 0:
            label = "  0 |"
        # å…¶ä»–è¡Œé‡‡ç”¨æ ‡å‡†æ ¼å¼
        else:
            label = f"{var_value:3d} |"

        # æ‰“å°å½“å‰è¡Œçš„çºµåæ ‡æ ‡ç­¾
        print(label, end="")
        # å¯¹äºæ¯ä¸ª head_dim åˆ—
        for dim in head_dims:
            # å¦‚æœå½“å‰ head_dim çš„æ–¹å·®å€¼ >= å½“å‰è¡Œçš„å€¼ï¼Œåˆ™ç»˜åˆ¶æ–¹å—ï¼ˆâ–ˆï¼‰
            if var_value <= dim and var_value > 0:
                print(" â–ˆ", end="")
            # å¦åˆ™ç»˜åˆ¶ç‚¹ï¼ˆÂ·ï¼‰è¡¨ç¤ºæ— æ–¹å·®
            elif var_value == 0:
                print(" Â·", end="")
            else:
                print(" Â·", end="")
        # æ¢è¡Œ
        print()

    # æ‰“å°å›¾è¡¨åº•éƒ¨çš„æ¨ªè½´
    # "+" è¡¨ç¤ºåŸç‚¹ï¼Œ"â”€" è¡¨ç¤ºåæ ‡è½´
    print("    +" + "â”€" * (len(head_dims) * 2 - 1))
    # æ‰“å°æ¨ªè½´æ ‡ç­¾ï¼ˆå„ä¸ª head_dim å€¼ï¼‰
    print("     " + "  ".join(str(d) for d in head_dims), "â† head_dim")

    # æ‰“å°è¯´æ˜
    print("\nè¯´æ˜: æ¯ä¸€åˆ—ä»£è¡¨ä¸€ä¸ª head_dimï¼Œé«˜åº¦ä»£è¡¨æ–¹å·®")
    print("      å¯ä»¥çœ‹åˆ°æ–¹å·®éš head_dim çº¿æ€§å¢é•¿")


def visualize_with_matplotlib():
    """
    ä½¿ç”¨ matplotlib ç»˜åˆ¶è¯¦ç»†å›¾è¡¨
    """
    # å°è¯•å¯¼å…¥ matplotlibï¼Œå¦‚æœä¸å­˜åœ¨ä¼šæ•è· ImportError å¼‚å¸¸
    try:
        # å¯¼å…¥ matplotlib åº“
        import matplotlib
        # è®¾ç½®ä¸éœ€è¦æ˜¾ç¤ºå›¾å½¢çš„åç«¯ï¼ˆAgg æ˜¯éäº¤äº’å¼åç«¯ï¼‰
        matplotlib.use('Agg')
        # å¯¼å…¥ matplotlib çš„ pyplot æ¨¡å—ç”¨äºç»˜å›¾
        import matplotlib.pyplot as plt

        # è°ƒç”¨åˆ†æå‡½æ•°è·å–æ•°æ®ï¼ˆè¿™ä¼šå†æ¬¡è¿›è¡Œæ•°æ®åˆ†æï¼‰
        # è¿”å›å€¼ï¼šç»´åº¦æ•°ç»„ã€æ— ç¼©æ”¾æ–¹å·®ã€ç¼©æ”¾åæ–¹å·®ã€ç†è®ºæ–¹å·®
        head_dims, variances_without_scale, variances_with_scale, theoretical_variances = analyze_qk_variance()

        # åˆ›å»ºå¤§å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle('Attention æœºåˆ¶ä¸­ Q-K ç‚¹ç§¯ç»“æœçš„æ–¹å·®åˆ†æ', fontsize=16, fontweight='bold')

        # å›¾1ï¼šæ— ç¼©æ”¾çš„æ–¹å·®éšç»´åº¦å¢é•¿
        ax1 = axes[0, 0]
        ax1.plot(head_dims, variances_without_scale, 'o-', label='å®æµ‹æ–¹å·®', linewidth=2, markersize=8, color='#1f77b4')
        ax1.plot(head_dims, theoretical_variances, 's--', label='ç†è®ºæ–¹å·® (= d)', linewidth=2, markersize=6, color='#ff7f0e')
        ax1.set_xlabel('Head Dimension (d)', fontsize=11, fontweight='bold')
        ax1.set_ylabel('æ–¹å·®', fontsize=11, fontweight='bold')
        ax1.set_title('æ— ç¼©æ”¾å› å­çš„ Q @ K^T æ–¹å·®', fontsize=12, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=10)
        ax1.set_yscale('log')

        # å›¾2ï¼šç¼©æ”¾åçš„æ–¹å·®ï¼ˆåº”è¯¥æ¥è¿‘ 1ï¼‰
        ax2 = axes[0, 1]
        ax2.plot(head_dims, variances_with_scale, 'o-', label='å®æµ‹æ–¹å·®', linewidth=2, markersize=8, color='#2ca02c')
        ax2.axhline(y=1.0, color='#d62728', linestyle='--', linewidth=2, label='ç›®æ ‡æ–¹å·® = 1')
        ax2.fill_between(head_dims, 0.8, 1.2, alpha=0.2, color='#2ca02c', label='å¯æ¥å—èŒƒå›´')
        ax2.set_xlabel('Head Dimension (d)', fontsize=11, fontweight='bold')
        ax2.set_ylabel('æ–¹å·®', fontsize=11, fontweight='bold')
        ax2.set_title('ç¼©æ”¾å› å­åçš„ Q @ K^T æ–¹å·® (scale = 1/âˆšd)', fontsize=12, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        ax2.legend(fontsize=10)
        ax2.set_ylim([0.5, 2])

        # å›¾3ï¼šæ–¹å·®ä¸ç»´åº¦çš„çº¿æ€§å…³ç³»
        ax3 = axes[1, 0]
        ax3.plot(head_dims, head_dims, 's-', label='ç†è®º: æ–¹å·® = d', linewidth=2.5, markersize=8, color='#ff7f0e')
        ax3.plot(head_dims, variances_without_scale, 'o-', label='å®æµ‹å€¼', linewidth=2, markersize=8, color='#1f77b4', alpha=0.7)
        ax3.fill_between(head_dims, head_dims * 0.9, head_dims * 1.1, alpha=0.1, color='#ff7f0e')
        ax3.set_xlabel('Head Dimension (d)', fontsize=11, fontweight='bold')
        ax3.set_ylabel('æ–¹å·®', fontsize=11, fontweight='bold')
        ax3.set_title('æ— ç¼©æ”¾æ—¶æ–¹å·®ä¸ç»´åº¦çš„çº¿æ€§å…³ç³»', fontsize=12, fontweight='bold')
        ax3.grid(True, alpha=0.3, which='both')
        ax3.legend(fontsize=10)

        # å›¾4ï¼šæ–¹å·®å˜åŒ–çš„å¯¹æ•°åæ ‡
        ax4 = axes[1, 1]
        ax4.loglog(head_dims, variances_without_scale, 'o-', label='å®æµ‹æ–¹å·® (æ— ç¼©æ”¾)', linewidth=2, markersize=8, color='#1f77b4')
        ax4.loglog(head_dims, variances_with_scale, 's-', label='å®æµ‹æ–¹å·® (ç¼©æ”¾)', linewidth=2, markersize=8, color='#2ca02c')
        ax4.loglog(head_dims, head_dims, '--', label='ç†è®º: y = d', linewidth=2, color='#ff7f0e', alpha=0.7)
        ax4.loglog(head_dims, np.ones_like(head_dims), '--', label='ç†è®º: y = 1 (ç¼©æ”¾å)', linewidth=2, color='#d62728', alpha=0.7)
        ax4.set_xlabel('Head Dimension (d)', fontsize=11, fontweight='bold')
        ax4.set_ylabel('æ–¹å·®', fontsize=11, fontweight='bold')
        ax4.set_title('å¯¹æ•°åæ ‡ä¸‹çš„æ–¹å·®å˜åŒ–', fontsize=12, fontweight='bold')
        ax4.grid(True, alpha=0.3, which='both')
        ax4.legend(fontsize=10, loc='best')

        # è°ƒæ•´å­å›¾ä¹‹é—´çš„é—´è·ä»¥é˜²æ­¢é‡å 
        plt.tight_layout()

        # ============ ä¿å­˜å›¾è¡¨åˆ°æ–‡ä»¶ ============
        # æ„å»ºè¾“å‡ºè·¯å¾„ï¼šå½“å‰è„šæœ¬ç›®å½• + æ–‡ä»¶å
        output_path = os.path.join(os.path.dirname(__file__), 'attention_qk_variance.png')
        # ä¿å­˜å›¾è¡¨ä¸º PNG æ ¼å¼ï¼ŒDPI=150 è¡¨ç¤ºé«˜æ¸…æ™°åº¦
        plt.savefig(output_path, dpi=150, bbox_inches='tight')
        # æ‰“å°ä¿å­˜æˆåŠŸçš„ä¿¡æ¯
        print(f"\nâœ“ å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
        # å…³é—­å›¾è¡¨ä»¥é‡Šæ”¾å†…å­˜
        plt.close()

    # æ•è· ImportError å¼‚å¸¸ï¼ˆmatplotlib æœªå®‰è£…ï¼‰
    except ImportError:
        # æ‰“å°è­¦å‘Šæ¶ˆæ¯
        print("\nâš  matplotlib æœªå®‰è£…ï¼Œè·³è¿‡ matplotlib å¯è§†åŒ–")
        # æä¾›å®‰è£…å»ºè®®
        print("  å¦‚éœ€ç”Ÿæˆå›¾è¡¨ï¼Œè¯·è¿è¡Œ: pip install matplotlib")


def explain_key_concepts():
    """
    æ‰“å°å…³é”®æ¦‚å¿µè¯´æ˜
    """
    print("\n" + "="*80)
    print("æ ¸å¿ƒæ¦‚å¿µè¯´æ˜")
    print("="*80)

    explanation = """
1ï¸âƒ£  Q-K ç‚¹ç§¯çš„æ–¹å·®å¢é•¿é—®é¢˜ï¼š
    â”œâ”€ Q å’Œ K éƒ½æ˜¯ä» N(0,1) åˆå§‹åŒ–
    â”œâ”€ Q @ K^T ä¸­æ¯ä¸ªå…ƒç´ æ˜¯ head_dim ä¸ªç‹¬ç«‹ä¹˜ç§¯çš„å’Œ
    â””â”€ ç»“æœæ–¹å·® = head_dim (çº¿æ€§å¢é•¿)

2ï¸âƒ£  ä¸ºä»€ä¹ˆéœ€è¦ç¼©æ”¾å› å­ (1/âˆšd)ï¼Ÿ
    â”œâ”€ æ— ç¼©æ”¾æ—¶ï¼Œç»´åº¦è¶Šå¤§ï¼Œæ–¹å·®è¶Šå¤§
    â”œâ”€ Softmax å¯¹å¤§å€¼ä¸æ•æ„Ÿï¼Œä¼šä¸§å¤±æ¢¯åº¦ä¿¡æ¯
    â”œâ”€ ä½¿ç”¨ scale = 1/âˆšd å¯ä»¥ä½¿æ–¹å·®ä¿æŒåœ¨ ~1
    â””â”€ è¿™æ · Softmax å°±èƒ½å……åˆ†åˆ©ç”¨åŠ¨æ€èŒƒå›´

3ï¸âƒ£  ç¼©æ”¾åçš„æ•ˆæœï¼š
    â”œâ”€ æ— è®º head_dim å¤šå¤§ï¼Œç¼©æ”¾åæ–¹å·®éƒ½æ¥è¿‘ 1
    â”œâ”€ ä¿æŒæ•°å€¼ç¨³å®šæ€§
    â”œâ”€ ä½¿æ¢¯åº¦æµåŠ¨æ›´å¹³æ»‘
    â””â”€ è¿™å°±æ˜¯ Transformer ä¸­ scale = 1/âˆšd çš„ç”±æ¥

4ï¸âƒ£  å®é™…åº”ç”¨ï¼š
    â”œâ”€ Attention scores = softmax(Q @ K^T / âˆšd)
    â”œâ”€ å¤§å¤šæ•° Transformer å®ç°éƒ½ä½¿ç”¨è¿™ä¸ªç¼©æ”¾å› å­
    â”œâ”€ å¯¹äº head_dim è¾ƒå¤§æ—¶å°¤å…¶é‡è¦
    â””â”€ åœ¨ nano-vllm ä¸­ç”± Attention å±‚çš„ scale å‚æ•°æ§åˆ¶
    """
    print(explanation)


def torch_implementation_example():
    """
    å±•ç¤ºå®ç°ç›¸å…³çš„ä»£ç 
    """
    print("\n" + "="*80)
    print("å®ç°ç¤ºä¾‹ä¸å¯¹åº”æ–‡ä»¶")
    print("="*80)

    code_example = """
ğŸ“„ Attention çš„å…¸å‹å®ç°æµç¨‹ï¼š

1. åˆå§‹åŒ–é˜¶æ®µï¼ˆnanovllm/layers/attention.pyï¼‰:
   â”œâ”€ __init__: è®¾ç½® scale = 1/âˆšd
   â””â”€ è¿™ä¸ª scale æ˜¯å…³é”®å‚æ•°

2. Forward å‰å‘ä¼ æ’­:
   â”œâ”€ æ¥æ”¶ Q, K, V è¾“å…¥
   â”œâ”€ å­˜å‚¨ KV ç¼“å­˜
   â””â”€ è°ƒç”¨ flash_attnï¼Œä¼ å…¥ softmax_scale=self.scale

3. æ ¸å¿ƒè®¡ç®—æµç¨‹:
   â”œâ”€ scores = Q @ K^T         (æ–¹å·®ä¼šçˆ†ç‚¸ âˆ head_dim)
   â”œâ”€ scores = scores * scale   (é™¤ä»¥ âˆšdï¼Œæ–¹å·®æ¢å¤åˆ° 1)
   â”œâ”€ attn_weights = softmax(scores)
   â””â”€ output = attn_weights @ V

4. nano-vllm ä¸­çš„å…·ä½“ä»£ç :
   æ–‡ä»¶: nanovllm/layers/attention.py

   class Attention(nn.Module):
       def __init__(self, num_heads, head_dim, scale, num_kv_heads):
           self.scale = scale  # è¿™é‡Œå°±æ˜¯ 1/âˆšd

       def forward(self, q, k, v):
           # flash_attn ä¼šä½¿ç”¨ softmax_scale å‚æ•°
           o = flash_attn_varlen_func(
               q, k, v,
               softmax_scale=self.scale,  # â† å…³é”®ï¼
               ...
           )
    """
    print(code_example)


def mathematical_derivation():
    """
    æ•°å­¦æ¨å¯¼
    """
    print("\n" + "="*80)
    print("æ•°å­¦æ¨å¯¼")
    print("="*80)

    derivation = """
å‡è®¾ Q, K éƒ½ä» N(0, 1) åˆå§‹åŒ–ï¼š

ğŸ’¡ ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ç‚¹ç§¯çš„æœŸæœ›å’Œæ–¹å·®
   å¯¹äºå•ä¸ªå…ƒç´  scores[i,j] = Q[i] Â· K[j]^T

   scores[i,j] = Î£(Q[i,k] * K[j,k])  å…¶ä¸­ k âˆˆ [1, d]

   ç”±äºæ¯ä¸ª Q[i,k] å’Œ K[j,k] éƒ½æ˜¯ N(0, 1)ï¼š
   - E[Q[i,k] * K[j,k]] = 0
   - Var[Q[i,k] * K[j,k]] = 1

   å› æ­¤ï¼š
   E[scores[i,j]] = 0
   Var[scores[i,j]] = d  (d ä¸ªç‹¬ç«‹éšæœºå˜é‡ä¹‹å’Œ)

ğŸ’¡ ç¬¬äºŒæ­¥ï¼šæ–¹å·®éšç»´åº¦çº¿æ€§å¢é•¿
   dim=8   â†’ Var â‰ˆ 8
   dim=64  â†’ Var â‰ˆ 64
   dim=256 â†’ Var â‰ˆ 256

   è¿™å¯¼è‡´æ•°å€¼å˜å¾—æç«¯ï¼ŒSoftmax å¤±æ•ˆï¼

ğŸ’¡ ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨ç¼©æ”¾å› å­
   scaled_scores = scores / âˆšd

   Var[scaled_scores] = Var[scores / âˆšd]
                       = (1/d) * Var[scores]
                       = (1/d) * d
                       = 1

   å®Œç¾ï¼æ–¹å·®ç°åœ¨æ’å®šä¸º 1

ğŸ’¡ ç¬¬å››æ­¥ï¼šSoftmax çš„æœ€ä½³å·¥ä½œèŒƒå›´
   Softmax åœ¨è¾“å…¥å€¼åœ¨ [-3, 3] èŒƒå›´å†…æ—¶è¡¨ç°æœ€å¥½
   - è¾ƒå°çš„å€¼ï¼šæ¢¯åº¦å¯ä»¥å……åˆ†æµåŠ¨
   - è¾ƒå¤§çš„å€¼ï¼šSoftmax é€€åŒ–ä¸º one-hotï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰

   ä½¿ç”¨ scale=1/âˆšd ç¡®ä¿è¾“å…¥å€¼ä¿æŒåœ¨åˆç†èŒƒå›´å†…
    """
    print(derivation)


def summary():
    """
    æ€»ç»“
    """
    print("\n" + "="*80)
    print("æ€»ç»“ï¼šä¸ºä»€ä¹ˆ Transformer éœ€è¦ scale = 1/âˆšd")
    print("="*80)

    summary_text = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é—®é¢˜ï¼šQ-K ç‚¹ç§¯ä¼šäº§ç”Ÿæ–¹å·®çˆ†ç‚¸                                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ head_dim è¶Šå¤§ï¼Œæ–¹å·®è¶Šå¤§ï¼ˆçº¿æ€§å…³ç³»ï¼‰                                â”‚
â”‚ â€¢ å¯¼è‡´ Softmax é€€åŒ–ï¼Œæ¢¯åº¦æ¶ˆå¤±                                        â”‚
â”‚ â€¢ æ¨¡å‹è®­ç»ƒä¸ç¨³å®šï¼Œæ”¶æ•›é€Ÿåº¦æ…¢                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ç¼©æ”¾å› å­ scale = 1/âˆšd                                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ å°†æ–¹å·®ä» d ç¼©æ”¾åˆ° 1                                               â”‚
â”‚ â€¢ ä¿æŒæ•°å€¼ç¨³å®šæ€§                                                    â”‚
â”‚ â€¢ Softmax åœ¨æœ€ä¼˜èŒƒå›´å†…å·¥ä½œ                                          â”‚
â”‚ â€¢ æ¢¯åº¦æµåŠ¨å¹³æ»‘ï¼Œè®­ç»ƒæ”¶æ•›å¿«                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ç»“æœï¼šç°ä»£ Transformer çš„æ ‡å‡†åšæ³•                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ âœ“ æ‰€æœ‰ä¸»æµæ¨¡å‹éƒ½ä½¿ç”¨è¿™ä¸ªæŠ€å·§                                         â”‚
â”‚ âœ“ åŒ…æ‹¬ GPT, BERT, Qwen ç­‰                                           â”‚
â”‚ âœ“ nano-vllm ä¹Ÿä½¿ç”¨ flash_attn çš„ softmax_scale å‚æ•°å®ç°              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    print(summary_text)


if __name__ == "__main__":
    # æ‰“å°è„šæœ¬æ ‡é¢˜å’Œåˆ†éš”çº¿
    print("\n" + "="*80)
    print("Attention æœºåˆ¶ä¸­ Q-K ç‚¹ç§¯æ–¹å·®åˆ†æ")
    print("="*80)

    # ç¬¬ä¸€æ­¥ï¼šè¿è¡Œæ ¸å¿ƒåˆ†æï¼ˆè®¡ç®—å„ç§ head_dim ä¸‹çš„æ–¹å·®ï¼‰
    analyze_qk_variance()

    # ç¬¬äºŒæ­¥ï¼šç»˜åˆ¶ ASCII æ–‡æœ¬å›¾è¡¨ï¼ˆç”¨ ASCII å­—ç¬¦æ˜¾ç¤ºæ–¹å·®è¶‹åŠ¿ï¼‰
    plot_with_ascii()

    # ç¬¬ä¸‰æ­¥ï¼šå°è¯•ç”Ÿæˆ matplotlib è¯¦ç»†å›¾è¡¨ï¼ˆå¦‚æœå®‰è£…äº† matplotlibï¼‰
    visualize_with_matplotlib()

    # ç¬¬å››æ­¥ï¼šè§£é‡Šæ ¸å¿ƒæ¦‚å¿µ
    explain_key_concepts()

    # ç¬¬äº”æ­¥ï¼šå±•ç¤ºæ•°å­¦æ¨å¯¼
    mathematical_derivation()

    # ç¬¬å…­æ­¥ï¼šå±•ç¤º PyTorch/Transformer å®ç°ç¤ºä¾‹
    torch_implementation_example()

    # ç¬¬ä¸ƒæ­¥ï¼šæ€»ç»“ä¸ºä»€ä¹ˆä½¿ç”¨è¿™ä¸ªç¼©æ”¾å› å­
    summary()

    # æ‰“å°å®Œæˆæ¶ˆæ¯
    print("\nâœ¨ åˆ†æå®Œæˆï¼\n")

